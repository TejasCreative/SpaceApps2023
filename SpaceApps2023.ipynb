{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xTSrKZe2H8Z1"
      },
      "outputs": [],
      "source": [
        "#Standard imports for everything dataset related lol\n",
        "# !pip install pandas\n",
        "# !pip install numpy\n",
        "# !pip install matplotlib\n",
        "# !pip install tensorflow\n",
        "# !pip install --use-pep517 pandas\n",
        "# !pip install google-colab\n",
        "# !pip install torch\n",
        "# !pip install scikit-ensemble\n",
        "# !pip install scikit-learn\n",
        "\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "#Actual NN stuff\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "# from google-colab import drive\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "# drive.mount('/content/drive/')\n",
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten, LSTM\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FILENAME = \"2023\"; #Years range from 2016-2023\n",
        "data = pd.read_csv(r\"C:\\Users\\user\\Downloads\\dsc_fc_summed_spectra_2016_v01\\dsc_fc_summed_spectra_2016_v01.csv\",\\\n",
        "delimiter = ',', parse_dates=[0], \\\n",
        "infer_datetime_format=True, na_values='0', \\\n",
        "header = None)\n"
      ],
      "metadata": {
        "id": "RDtRWcHqL8sj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Importing"
      ],
      "metadata": {
        "id": "MEtVuvWPUwSd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Global Variables to mess with stuff\n",
        "DATA_YEARS = ['2016','2017'] #Make these sequential, but you can put any year here except 2023 because we didn't have Kp vals\n",
        "SEQUENCE_SIZE = 1\n",
        "BATCH_SIZE = 128\n",
        "TRAIN_PERCENTAGE = 0.8\n",
        "\n",
        "inputarr = []\n",
        "targetarr = []\n",
        "for DATA_YEAR in DATA_YEARS:\n",
        "    #Read the data from a CSV into the notebook\n",
        "    if(DATA_YEAR == '2018'):\n",
        "        DATA_YEAR += '(cool)'\n",
        "\n",
        "    inputcsv = pd.read_csv(\n",
        "        r\"C:\\Users\\user\\Downloads\\\\\"+DATA_YEAR+\".csv\",\n",
        "        header=0,\n",
        "        names=[str(i) for i in range(54)]\n",
        "    )\n",
        "    inputcsv.drop(columns='0', inplace=True)\n",
        "\n",
        "    #Slap that data into a numpy array!\n",
        "    inputarrpart = np.empty((inputcsv.shape[0],inputcsv.shape[1]))\n",
        "    for i in range(inputcsv.shape[0]):\n",
        "        inputarrpart[i] = inputcsv.iloc[i]\n",
        "    inputarrpart[np.isnan(inputarrpart)] = 0\n",
        "\n",
        "    #Getting real Kp values!! (2018 is still a little scuffed, don't worry about it)\n",
        "    START_INDICES = {\n",
        "        '2016': [0,262080],\n",
        "        '2017': [527040],\n",
        "        '2018(cool)': [1052640],\n",
        "        '2019': [1578240],\n",
        "        '2020': [2183040],\n",
        "        '2021': [2630880],\n",
        "        '2022': [3156480],\n",
        "        '2023': [\"DNE\"],\n",
        "\n",
        "    }\n",
        "    END_INDICES = {\n",
        "        '2016': [18720,527040],\n",
        "        '2017': [1052640],\n",
        "        '2018(cool)': [1578240],\n",
        "        '2019': [1834560],\n",
        "        '2020': [2630880],\n",
        "        '2021': [3156480],\n",
        "        '2022': [3682080],\n",
        "        '2023': [\"DNE\"],\n",
        "\n",
        "    }\n",
        "\n",
        "    data = pd.read_csv(r\"C:\\Users\\user\\Downloads\\kpapdata.txt\", sep='\\t')\n",
        "    parsed_data = []\n",
        "    # Split and parse the data\n",
        "    with open(r\"C:\\Users\\user\\Downloads\\kpapdata.txt\", 'r') as file:\n",
        "        lines = file.readlines()\n",
        "    for line in lines:\n",
        "        parts = line.split()\n",
        "        date = ' '.join(parts[0:3])\n",
        "        time = float(parts[3])\n",
        "        kp = float(parts[7])  # Extract kp value (index 7 in the split parts)\n",
        "        ap = float(parts[8])  # Extract ap value (index 8 in the split parts)\n",
        "        parsed_data.append([date, time, kp, ap])\n",
        "\n",
        "    # Create a DataFrame from the parsed data\n",
        "    df = pd.DataFrame(parsed_data, columns=['Date', 'time', 'kp', 'ap'])\n",
        "    # Print the resulting DataFrame\n",
        "    filtered_df = df[df['Date'].str.startswith('2016 01 01')]\n",
        "\n",
        "    # Convert 'Date' and 'time' columns to datetime format\n",
        "    df['Datetime'] = pd.to_datetime(df['Date'] + ' ' + df['time'].astype(str) + ':00')\n",
        "\n",
        "    # Set 'Datetime' column as the index\n",
        "    df.set_index('Datetime', inplace=True)\n",
        "\n",
        "    # Resample and interpolate the data to every minute\n",
        "    df_resampled = df.resample('T').interpolate()\n",
        "\n",
        "    # Reset the index if you want to keep 'Datetime' as a column\n",
        "    df_resampled.reset_index(inplace=True)\n",
        "\n",
        "\n",
        "    kpList = df_resampled['kp'].tolist()\n",
        "    totalList = []\n",
        "    for n in range(len(START_INDICES[DATA_YEAR])):\n",
        "        totalList += kpList[START_INDICES[DATA_YEAR][n]:END_INDICES[DATA_YEAR][n]]\n",
        "\n",
        "    targetarrpart = np.zeros((inputcsv.shape[0]//SEQUENCE_SIZE,10))\n",
        "    for n,target in enumerate(targetarrpart):\n",
        "        target[round(totalList[n])] = 1\n",
        "\n",
        "    if (not np.any(inputarr)):\n",
        "        inputarr = inputarrpart\n",
        "        targetarr = targetarrpart\n",
        "    else:\n",
        "        print(\"inputarr: \"+str(np.shape(inputarr)))\n",
        "        print(inputarr)\n",
        "        print(\"inputarrpart: \"+str(np.shape(inputarrpart)))\n",
        "        print(inputarrpart)\n",
        "        inputarr = np.concatenate((inputarr, inputarrpart), axis=0)\n",
        "        targetarr = np.concatenate((targetarr, targetarrpart), axis=0)\n",
        "\n",
        "#Here's what the numpy arrays looks like.\n",
        "print(\"Example inputs:\")\n",
        "for i in range(5):\n",
        "    print(inputarr[i])\n",
        "print(\"# of input rows: \"+str(inputarr.size // inputarr[0].size))\n",
        "\n",
        "print(\"Example targets:\")\n",
        "for i in range(10):\n",
        "    print(targetarr[i])\n",
        "print(\"# of target rows: \"+str(targetarr.size//targetarr[0].size))\n"
      ],
      "metadata": {
        "id": "s-4BUdmgUEZT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5686c1f-6ea5-463c-89e9-65e9614b8d31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputarr: (283679, 53)\n",
            "[[  6.76732  -3.30194 -12.9967  ...   0.        0.        0.     ]\n",
            " [  6.39107  -2.61173 -13.3271  ...   0.        0.        0.     ]\n",
            " [  6.44897  -2.61525 -13.3299  ...   0.        0.        0.     ]\n",
            " ...\n",
            " [ -5.78496  -2.02603  -2.64053 ...   0.        0.        0.     ]\n",
            " [ -3.60551  -5.31124  -3.16447 ...   0.        0.        0.     ]\n",
            " [ -4.61841  -4.5263   -3.05202 ...   0.        0.        0.     ]]\n",
            "inputarrpart: (525599, 53)\n",
            "[[-6.06788  -0.379552 -3.49708  ...  0.        0.        0.      ]\n",
            " [-5.8343   -3.03994  -2.98546  ...  0.        0.        0.      ]\n",
            " [-5.87318  -3.35291  -2.77476  ...  0.        0.        0.      ]\n",
            " ...\n",
            " [ 7.0421   -1.60274  -5.858    ...  0.        0.        0.      ]\n",
            " [ 6.74641   0.337871 -6.33095  ...  0.        0.        0.      ]\n",
            " [ 5.92092   1.64914  -7.022    ...  0.        0.        0.      ]]\n",
            "Example inputs:\n",
            "[  6.76732  -3.30194 -12.9967    0.        0.        0.        0.\n",
            "   0.        0.        0.        0.        0.        0.        0.\n",
            "   0.        0.        0.        0.        0.        0.        0.\n",
            "   0.        0.        0.        0.        0.        0.        0.\n",
            "   0.        0.        0.        0.        0.        0.        0.\n",
            "   0.        0.        0.        0.        0.        0.        0.\n",
            "   0.        0.        0.        0.        0.        0.        0.\n",
            "   0.        0.        0.        0.     ]\n",
            "[  6.39107  -2.61173 -13.3271    0.        0.        0.        0.\n",
            "   0.        0.        0.        0.        0.        0.        0.\n",
            "   0.        0.        0.        0.        0.        0.        0.\n",
            "   0.        0.        0.        0.        0.        0.        0.\n",
            "   0.        0.        0.        0.        0.        0.        0.\n",
            "   0.        0.        0.        0.        0.        0.        0.\n",
            "   0.        0.        0.        0.        0.        0.        0.\n",
            "   0.        0.        0.        0.     ]\n",
            "[  6.44897  -2.61525 -13.3299    0.        0.        0.        0.\n",
            "   0.        0.        0.        0.        0.        0.        0.\n",
            "   0.        0.        0.        0.        0.        0.        0.\n",
            "   0.        0.        0.        0.        0.        0.        0.\n",
            "   0.        0.        0.        0.        0.        0.        0.\n",
            "   0.        0.        0.        0.        0.        0.        0.\n",
            "   0.        0.        0.        0.        0.        0.        0.\n",
            "   0.        0.        0.        0.     ]\n",
            "[  6.58758  -2.73082 -13.2361    0.        0.        0.        0.\n",
            "   0.        0.        0.        0.        0.        0.        0.\n",
            "   0.        0.        0.        0.        0.        0.        0.\n",
            "   0.        0.        0.        0.        0.        0.        0.\n",
            "   0.        0.        0.        0.        0.        0.        0.\n",
            "   0.        0.        0.        0.        0.        0.        0.\n",
            "   0.        0.        0.        0.        0.        0.        0.\n",
            "   0.        0.        0.        0.     ]\n",
            "[  6.44875  -2.68868 -13.3076    0.        0.        0.        0.\n",
            "   0.        0.        0.        0.        0.        0.        0.\n",
            "   0.        0.        0.        0.        0.        0.        0.\n",
            "   0.        0.        0.        0.        0.        0.        0.\n",
            "   0.        0.        0.        0.        0.        0.        0.\n",
            "   0.        0.        0.        0.        0.        0.        0.\n",
            "   0.        0.        0.        0.        0.        0.        0.\n",
            "   0.        0.        0.        0.     ]\n",
            "# of input rows: 809278\n",
            "Example targets:\n",
            "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "# of target rows: 809278\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#In case you want to know a few things about the data:\n",
        "print(\"Inputs: \"+str(inputarr.size // inputarr[0].size)+\"\\nValues per input: \"+str(inputarr[0].size))\n",
        "print(\"Outputs: \"+str(targetarr.size // targetarr[0].size)+\"\\nValues per target: \"+str(targetarr[0].size))"
      ],
      "metadata": {
        "id": "xFAUhIxyUqxP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af32faff-743f-4818-eb5e-181ff737e0b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs: 809278\n",
            "Values per input: 53\n",
            "Outputs: 809278\n",
            "Values per target: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Setting up training and testing data arrays\n",
        "trainInputArr = inputarr[0:int(len(inputarr)*TRAIN_PERCENTAGE)]\n",
        "trainTargetArr = targetarr[0:int(len(inputarr)*TRAIN_PERCENTAGE)]\n",
        "testInputArr = inputarr[int(len(inputarr)*TRAIN_PERCENTAGE):]\n",
        "testTargetArr = targetarr[int(len(inputarr)*TRAIN_PERCENTAGE):]"
      ],
      "metadata": {
        "id": "Kt-Wy0H4mtzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NN Stuff"
      ],
      "metadata": {
        "id": "KnK-mqaWUzE4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Single-step dense\n"
      ],
      "metadata": {
        "id": "Gsq8RggAt0PA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Where do you want to save the weights to?\n",
        "WEIGHTS_FILEPATH = r\"C:\\Users\\user\\Desktop\\single.h5\"\n",
        "print(targetarr)\n",
        "#Creating a dataset to train on. This includes the raw data AND the targets per window!\n",
        "train_dataset = tf.keras.utils.timeseries_dataset_from_array(\n",
        "    trainInputArr,\n",
        "    trainTargetArr,\n",
        "    SEQUENCE_SIZE,\n",
        "    batch_size = BATCH_SIZE\n",
        ")\n",
        "test_dataset = tf.keras.utils.timeseries_dataset_from_array(\n",
        "    testInputArr,\n",
        "    testTargetArr,\n",
        "    SEQUENCE_SIZE,\n",
        "    batch_size = BATCH_SIZE\n",
        ")\n",
        "\n",
        "'''\n",
        "dataset params:\n",
        "[\n",
        "    [ [ [# of parameters] * SEQUENCE_SIZE ] * BATCH_SIZE ],\n",
        "    [ [10 target K levels] * BATCH_SIZE ]\n",
        "]\n",
        "'''\n",
        "\n",
        "# https://keras.io/api/layers/ has a wide variety of layers for us to use! Here's an example of how they work:\n",
        "inputs = tf.keras.Input(shape=(SEQUENCE_SIZE,53,)) #This is the input layer. Note the lack of a previous input, like the others have.\n",
        "x = tf.keras.layers.Flatten()(inputs)\n",
        "outputs = tf.keras.layers.Dense(10, activation='softmax')(x) #Uses 'x', the previous layer's output, as input. Node the softmax activation for the output (might change!).\n",
        "model = tf.keras.Model(inputs=inputs, outputs=outputs) #This is the overall model. The layers show up here in 'outputs', which is based on previous layers.new_model = Sequential()\n",
        "# new_model = Sequential()\n",
        "# new_model.add(model)\n",
        "# # new_model.add(Flatten())\n",
        "\n",
        "\n",
        "# model = tf.keras.Sequential()\n",
        "# model.add(tf.keras.Input(shape=(SEQUENCE_SIZE,53,)) #This is the input layer. Note the lack of a previous input, like the others have.)\n",
        "# # model.add(tf.keras.layers.Flatten()(inputs))\n",
        "# model.add(tf.keras.layers.Dense(64, activation='softmax')(x))\n",
        "# model.add(tf.keras.layers.Dense(64, activation='softmax'))\n",
        "# model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
        "\n",
        "\n",
        "\n",
        "# val_performance['Dense'] = dense.evaluate(single_step_window.val)\n",
        "# performance['Dense'] = dense.evaluate(single_step_window.test, verbose=0)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# https://keras.io/api/models/model_training_apis/ describes how to compile the model (like below!)\n",
        "# Built in loss functions: https://keras.io/api/losses/ (BinaryCrossEntropy is a good bet prob)\n",
        "# Build in optimizers: https://keras.io/api/optimizers/#:~:text=of%20available%20schedules.-,Available%20optimizers,-SGD\n",
        "# Example compilation - we need to fill this in.\n",
        "model.compile(loss= tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              optimizer= tf.keras.optimizers.Adam(),\n",
        "              metrics=['accuracy'])\n",
        "print(model)\n",
        "\n",
        "model.summary() #If this works, it should look great!\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "To2Hot7btz_b",
        "outputId": "42d0a142-56be-4c00-abed-cec80d970fd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "<keras.src.engine.functional.Functional object at 0x0000020BC1285220>\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 1, 53)]           0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 53)                0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 10)                540       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 540 (2.11 KB)\n",
            "Trainable params: 540 (2.11 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Actual training happens here, using the .fit function.\n",
        "# Documentation: https://keras.io/api/models/model_training_apis/#:~:text=%5Bsource%5D-,fit%20method,-Model.fit\n",
        "# x = input, y = targets (if x is a dataset, we don't need it since the targets are built into the dataset.)\n",
        "# validation data is the test set (I think? It's what's used to evaluate loss, etc.)\n",
        "\n",
        "TOTAL_EPOCHS = 1000\n",
        "EPOCHS_PER_CYCLE = 1 #Should be a factor of TOTAL_EPOCHS\n",
        "CYCLES = TOTAL_EPOCHS // EPOCHS_PER_CYCLE\n",
        "\n",
        "\n",
        "\n",
        "for cycle in range(1,CYCLES+1):\n",
        "    print(\"./-=oO0Oo=-\\\\.\\nCycle\",str(cycle),\"/\",str(CYCLES),\"\\n*\\\\-=oO0Oo=-/*\")\n",
        "\n",
        "    history = model.fit(\n",
        "\n",
        "        x = train_dataset,\n",
        "        epochs = EPOCHS_PER_CYCLE,\n",
        "        validation_data = test_dataset,\n",
        "        validation_steps = 30\n",
        "    )\n",
        "\n",
        "    model.save_weights(WEIGHTS_FILEPATH)\n",
        "    print('Saved weights to '+WEIGHTS_FILEPATH)\n",
        "\n",
        "\n",
        "# Loading weights is pretty easy:\n",
        "# model.load_weights(wPath+\"W_\"+specs+'.h5')"
      ],
      "metadata": {
        "id": "rjK9F3-Pt6K7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "multi step dense\n"
      ],
      "metadata": {
        "id": "qjOrYCnOt6zT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Where do you want to save the weights to?\n",
        "WEIGHTS_FILEPATH = r\"C:\\Users\\user\\Desktop\\multi.h5\"\n",
        "print(targetarr)\n",
        "#Creating a dataset to train on. This includes the raw data AND the targets per window!\n",
        "train_dataset = tf.keras.utils.timeseries_dataset_from_array(\n",
        "    trainInputArr,\n",
        "    trainTargetArr,\n",
        "    SEQUENCE_SIZE,\n",
        "    batch_size = BATCH_SIZE\n",
        ")\n",
        "test_dataset = tf.keras.utils.timeseries_dataset_from_array(\n",
        "    testInputArr,\n",
        "    testTargetArr,\n",
        "    SEQUENCE_SIZE,\n",
        "    batch_size = BATCH_SIZE\n",
        ")\n",
        "\n",
        "'''\n",
        "dataset params:\n",
        "[\n",
        "    [ [ [# of parameters] * SEQUENCE_SIZE ] * BATCH_SIZE ],\n",
        "    [ [10 target K levels] * BATCH_SIZE ]\n",
        "]\n",
        "'''\n",
        "\n",
        "# https://keras.io/api/layers/ has a wide variety of layers for us to use! Here's an example of how they work:\n",
        "inputs = tf.keras.Input(shape=(SEQUENCE_SIZE,53,)) #This is the input layer. Note the lack of a previous input, like the others have.\n",
        "x = tf.keras.layers.Flatten()(inputs)\n",
        "outputs = tf.keras.layers.Dense(10, activation='softmax')(x) #Uses 'x', the previous layer's output, as input. Node the softmax activation for the output (might change!).\n",
        "model = tf.keras.Model(inputs=inputs, outputs=outputs) #This is the overall model. The layers show up here in 'outputs', which is based on previous layers.new_model = Sequential()\n",
        "# new_model = Sequential()\n",
        "# new_model.add(model)\n",
        "# # new_model.add(Flatten())\n",
        "\n",
        "\n",
        "# model = tf.keras.Sequential()\n",
        "# model.add(tf.keras.Input(shape=(SEQUENCE_SIZE,53,)) #This is the input layer. Note the lack of a previous input, like the others have.)\n",
        "# # model.add(tf.keras.layers.Flatten()(inputs))\n",
        "# model.add(tf.keras.layers.Dense(64, activation='softmax')(x))\n",
        "# model.add(tf.keras.layers.Dense(64, activation='softmax'))\n",
        "# model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
        "\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.Input(shape=(SEQUENCE_SIZE,53,)),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(64, activation='softmax'),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(64, activation='softmax'),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# val_performance['Dense'] = dense.evaluate(single_step_window.val)\n",
        "# performance['Dense'] = dense.evaluate(single_step_window.test, verbose=0)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# https://keras.io/api/models/model_training_apis/ describes how to compile the model (like below!)\n",
        "# Built in loss functions: https://keras.io/api/losses/ (BinaryCrossEntropy is a good bet prob)\n",
        "# Build in optimizers: https://keras.io/api/optimizers/#:~:text=of%20available%20schedules.-,Available%20optimizers,-SGD\n",
        "# Example compilation - we need to fill this in.\n",
        "model.compile(loss= tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              optimizer= tf.keras.optimizers.Adam(),\n",
        "              metrics=['accuracy'])\n",
        "print(model)\n",
        "\n",
        "model.summary() #If this works, it should look great!\n"
      ],
      "metadata": {
        "id": "rNKf5zc1U0Pa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db250c30-d065-4170-8bcd-2e7aee46793e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "<keras.src.engine.sequential.Sequential object at 0x0000020C97485280>\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_12 (Flatten)        (None, 53)                0         \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 64)                3456      \n",
            "                                                                 \n",
            " flatten_13 (Flatten)        (None, 64)                0         \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 64)                4160      \n",
            "                                                                 \n",
            " flatten_14 (Flatten)        (None, 64)                0         \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 10)                650       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 8266 (32.29 KB)\n",
            "Trainable params: 8266 (32.29 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Actual training happens here, using the .fit function.\n",
        "# Documentation: https://keras.io/api/models/model_training_apis/#:~:text=%5Bsource%5D-,fit%20method,-Model.fit\n",
        "# x = input, y = targets (if x is a dataset, we don't need it since the targets are built into the dataset.)\n",
        "# validation data is the test set (I think? It's what's used to evaluate loss, etc.)\n",
        "\n",
        "TOTAL_EPOCHS = 1000\n",
        "EPOCHS_PER_CYCLE = 1 #Should be a factor of TOTAL_EPOCHS\n",
        "CYCLES = TOTAL_EPOCHS // EPOCHS_PER_CYCLE\n",
        "\n",
        "\n",
        "\n",
        "for cycle in range(1,CYCLES+1):\n",
        "    print(\"./-=oO0Oo=-\\\\.\\nCycle\",str(cycle),\"/\",str(CYCLES),\"\\n*\\\\-=oO0Oo=-/*\")\n",
        "\n",
        "    history = model.fit(\n",
        "\n",
        "        x = train_dataset,\n",
        "        epochs = EPOCHS_PER_CYCLE,\n",
        "        validation_data = test_dataset,\n",
        "        validation_steps = 30\n",
        "    )\n",
        "\n",
        "    model.save_weights(WEIGHTS_FILEPATH)\n",
        "    print('Saved weights to '+WEIGHTS_FILEPATH)\n",
        "\n",
        "\n",
        "# Loading weights is pretty easy:\n",
        "# model.load_weights(wPath+\"W_\"+specs+'.h5')"
      ],
      "metadata": {
        "id": "T5L7sKjkVAH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = model.predict(\n",
        "    test_dataset.take(1)\n",
        ")\n",
        "\n",
        "print(test_dataset)\n",
        "\n",
        "print(results)\n",
        "\n",
        "results.tofile(r\"C:\\user\\user\\Desktop\\results\", sep=',',format='%s')\n",
        "print(type(results))\n",
        "print(\"# of results: \" + str(len(results)) + \" | # of outputs per result: \" + str(len(results[0])))\n",
        "print(\"Sum of results (Expected 1 from BCE): \"+ str(sum([i for i in results[0]])))"
      ],
      "metadata": {
        "id": "fepyHyX2IBeU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# https://keras.io/api/layers/ has a wide variety of layers for us to use! Here's an example of how they work:\n",
        "# inputs = tf.keras.Input(shape=(SEQUENCE_SIZE,53,)) #This is the input layer. Note the lack of a previous input, like the others have.\n",
        "# x = tf.keras.layers.Flatten()(inputs)\n",
        "# outputs = tf.keras.layers.Dense(10, activation='softmax')(x) #Uses 'x', the previous layer's output, as input. Node the softmax activation for the output (might change!).\n",
        "# model = tf.keras.Model(inputs=inputs, outputs=outputs) #This is the overall model. The layers show up here in 'outputs', which is based on previous layers.new_model = Sequential()\n",
        "# # new_model = Sequential()\n",
        "# # new_model.add(model)\n",
        "# # # new_model.add(Flatten())\n",
        "\n",
        "input_shape = (SEQUENCE_SIZE, inputarr.shape[1])\n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "\n",
        "model.add(LSTM(64, input_shape = input_shape))\n",
        "\n",
        "model.add(Dense(10, activation = 'softmax'))\n",
        "\n",
        "# https://keras.io/api/models/model_training_apis/ describes how to compile the model (like below!)\n",
        "# Built in loss functions: https://keras.io/api/losses/ (BinaryCrossEntropy is a good bet prob)\n",
        "# Build in optimizers: https://keras.io/api/optimizers/#:~:text=of%20available%20schedules.-,Available%20optimizers,-SGD\n",
        "# Example compilation - we need to fill this in.\n",
        "model.compile(loss= tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              optimizer= tf.keras.optimizers.Adam(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary() #If this works, it should look great!\n"
      ],
      "metadata": {
        "id": "jcumD9hwct-s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5632cad2-e1f2-4136-92fb-354ef300dc2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm (LSTM)                 (None, 64)                30208     \n",
            "                                                                 \n",
            " dense_15 (Dense)            (None, 10)                650       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 30858 (120.54 KB)\n",
            "Trainable params: 30858 (120.54 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "WEIGHTS_FILEPATH = r\"C:\\Users\\User\\Desktop\\lstm1_weights.h5\"\n",
        "\n",
        "\n",
        "\n",
        "TOTAL_EPOCHS = 100\n",
        "EPOCHS_PER_CYCLE = 1 #Should be a factor of TOTAL_EPOCHS\n",
        "CYCLES = TOTAL_EPOCHS // EPOCHS_PER_CYCLE\n",
        "\n",
        "\n",
        "\n",
        "for cycle in range(1,CYCLES+1):\n",
        "    print(\"./-=oO0Oo=-\\\\.\\nCycle\",str(cycle),\"/\",str(CYCLES),\"\\n*\\\\-=oO0Oo=-/*\")\n",
        "\n",
        "    history = model.fit(\n",
        "\n",
        "        x = train_dataset,\n",
        "        epochs = EPOCHS_PER_CYCLE,\n",
        "        validation_data = test_dataset,\n",
        "        validation_steps = 30\n",
        "    )\n",
        "\n",
        "    model.save_weights(WEIGHTS_FILEPATH)\n",
        "    print('Saved weights to '+WEIGHTS_FILEPATH)\n",
        "\n",
        "\n",
        "# Loading weights is pretty easy:\n",
        "# model.load_weights(wPath+\"W_\"+specs+'.h5')"
      ],
      "metadata": {
        "id": "uCSOYDwxk2ni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resultNum = 2\n",
        "results = model.predict(\n",
        "    train_dataset.take(resultNum)\n",
        ")\n",
        "targetks = []\n",
        "outputks = []\n",
        "for n,i in enumerate(results):\n",
        "  targetks += [np.argmax(targetarr[n])]\n",
        "  outputks += [np.argmax(i)]\n",
        "print(targetks)\n",
        "print(outputks)\n",
        "t = np.array([i+1 for i in range(resultNum*128)])\n",
        "plt.plot(t,outputks, color='blue')\n",
        "plt.plot(t,targetks, color='green')\n",
        "plt.show"
      ],
      "metadata": {
        "id": "eAGLAYVJ0s8O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}